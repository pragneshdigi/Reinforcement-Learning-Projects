{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8bf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572e4309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "sta : 4\n",
      "act : 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "sta = env.observation_space.shape[0]\n",
    "act = env.action_space.n\n",
    "\n",
    "print(\"env : {}\".format(env.observation_space))\n",
    "print (\"sta : {}\".format(sta))\n",
    "print (\"act : {}\".format(act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfcc2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b63b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sta, act):\n",
    "    model = Sequential()\n",
    "    #model.add(Flatten(input_shape=(1,sta)))\n",
    "    #model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu', input_shape=(1,sta)))\n",
    "    \n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(act, activation='linear'))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875f5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(sta,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c3eb0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1, 24)             120       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1, 24)             600       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1, 2)              50        \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d00b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f164f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71bf1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, act):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=act, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "217bb991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env310\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\ProgramData\\Anaconda3\\envs\\env310\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    14/50000: episode: 1, duration: 2.361s, episode steps:  14, steps per second:   6, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.654321, mae: 0.642532, mean_q: 0.117177\n",
      "    27/50000: episode: 2, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.490358, mae: 0.566530, mean_q: 0.208048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env310\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\ProgramData\\Anaconda3\\envs\\env310\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    44/50000: episode: 3, duration: 0.169s, episode steps:  17, steps per second: 101, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.282527, mae: 0.549140, mean_q: 0.419848\n",
      "    76/50000: episode: 4, duration: 0.257s, episode steps:  32, steps per second: 124, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  loss: 0.151480, mae: 0.606987, mean_q: 0.795578\n",
      "   104/50000: episode: 5, duration: 0.233s, episode steps:  28, steps per second: 120, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 0.096278, mae: 0.724436, mean_q: 1.206077\n",
      "   117/50000: episode: 6, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.091281, mae: 0.840382, mean_q: 1.458386\n",
      "   127/50000: episode: 7, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.106284, mae: 0.887852, mean_q: 1.525081\n",
      "   153/50000: episode: 8, duration: 0.267s, episode steps:  26, steps per second:  97, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.081008, mae: 0.921209, mean_q: 1.638139\n",
      "   163/50000: episode: 9, duration: 0.082s, episode steps:  10, steps per second: 122, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.057955, mae: 0.996478, mean_q: 1.928965\n",
      "   177/50000: episode: 10, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.055903, mae: 1.054583, mean_q: 2.047314\n",
      "   190/50000: episode: 11, duration: 0.152s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.059765, mae: 1.115963, mean_q: 2.166170\n",
      "   205/50000: episode: 12, duration: 0.173s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.085276, mae: 1.174608, mean_q: 2.287941\n",
      "   215/50000: episode: 13, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.092287, mae: 1.212187, mean_q: 2.326489\n",
      "   239/50000: episode: 14, duration: 0.191s, episode steps:  24, steps per second: 126, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.090711, mae: 1.276071, mean_q: 2.553439\n",
      "   253/50000: episode: 15, duration: 0.122s, episode steps:  14, steps per second: 115, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.133834, mae: 1.370347, mean_q: 2.623650\n",
      "   287/50000: episode: 16, duration: 0.270s, episode steps:  34, steps per second: 126, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.125776, mae: 1.447658, mean_q: 2.791848\n",
      "   299/50000: episode: 17, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.155549, mae: 1.557549, mean_q: 2.982893\n",
      "   326/50000: episode: 18, duration: 0.266s, episode steps:  27, steps per second: 102, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.148325, mae: 1.633512, mean_q: 3.142487\n",
      "   348/50000: episode: 19, duration: 0.214s, episode steps:  22, steps per second: 103, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.176632, mae: 1.738698, mean_q: 3.327533\n",
      "   368/50000: episode: 20, duration: 0.234s, episode steps:  20, steps per second:  85, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.181211, mae: 1.837314, mean_q: 3.521199\n",
      "   398/50000: episode: 21, duration: 0.298s, episode steps:  30, steps per second: 101, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.195073, mae: 1.927459, mean_q: 3.665426\n",
      "   421/50000: episode: 22, duration: 0.207s, episode steps:  23, steps per second: 111, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.197511, mae: 2.039311, mean_q: 3.926651\n",
      "   442/50000: episode: 23, duration: 0.179s, episode steps:  21, steps per second: 117, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.179197, mae: 2.121885, mean_q: 4.067672\n",
      "   457/50000: episode: 24, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.219298, mae: 2.228159, mean_q: 4.302717\n",
      "   467/50000: episode: 25, duration: 0.079s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.272109, mae: 2.239134, mean_q: 4.224056\n",
      "   479/50000: episode: 26, duration: 0.099s, episode steps:  12, steps per second: 121, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.257390, mae: 2.308616, mean_q: 4.435779\n",
      "   491/50000: episode: 27, duration: 0.108s, episode steps:  12, steps per second: 111, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.200487, mae: 2.327908, mean_q: 4.429018\n",
      "   502/50000: episode: 28, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.344384, mae: 2.431001, mean_q: 4.589520\n",
      "   518/50000: episode: 29, duration: 0.134s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.329352, mae: 2.434529, mean_q: 4.549741\n",
      "   550/50000: episode: 30, duration: 0.253s, episode steps:  32, steps per second: 127, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 0.286998, mae: 2.533189, mean_q: 4.789894\n",
      "   577/50000: episode: 31, duration: 0.216s, episode steps:  27, steps per second: 125, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.271030, mae: 2.652569, mean_q: 5.019650\n",
      "   606/50000: episode: 32, duration: 0.234s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.315120, mae: 2.736212, mean_q: 5.190818\n",
      "   616/50000: episode: 33, duration: 0.084s, episode steps:  10, steps per second: 119, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.272280, mae: 2.824316, mean_q: 5.371625\n",
      "   643/50000: episode: 34, duration: 0.234s, episode steps:  27, steps per second: 115, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.296873, mae: 2.886944, mean_q: 5.457467\n",
      "   660/50000: episode: 35, duration: 0.151s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.265957, mae: 2.991114, mean_q: 5.717940\n",
      "   677/50000: episode: 36, duration: 0.148s, episode steps:  17, steps per second: 115, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.366550, mae: 3.060131, mean_q: 5.811507\n",
      "   741/50000: episode: 37, duration: 0.542s, episode steps:  64, steps per second: 118, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.376711, mae: 3.182374, mean_q: 6.040734\n",
      "   758/50000: episode: 38, duration: 0.150s, episode steps:  17, steps per second: 114, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.332304, mae: 3.370597, mean_q: 6.477032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   790/50000: episode: 39, duration: 0.337s, episode steps:  32, steps per second:  95, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.366133, mae: 3.419968, mean_q: 6.562544\n",
      "   845/50000: episode: 40, duration: 0.553s, episode steps:  55, steps per second:  99, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.374032, mae: 3.595916, mean_q: 6.923237\n",
      "   876/50000: episode: 41, duration: 0.316s, episode steps:  31, steps per second:  98, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.465225, mae: 3.741570, mean_q: 7.181188\n",
      "   898/50000: episode: 42, duration: 0.210s, episode steps:  22, steps per second: 105, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.401562, mae: 3.869112, mean_q: 7.456728\n",
      "   930/50000: episode: 43, duration: 0.327s, episode steps:  32, steps per second:  98, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.426292, mae: 3.988158, mean_q: 7.737596\n",
      "   983/50000: episode: 44, duration: 0.441s, episode steps:  53, steps per second: 120, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.522735, mae: 4.117104, mean_q: 7.927917\n",
      "  1006/50000: episode: 45, duration: 0.184s, episode steps:  23, steps per second: 125, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.338321, mae: 4.282846, mean_q: 8.316570\n",
      "  1028/50000: episode: 46, duration: 0.174s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.565934, mae: 4.374843, mean_q: 8.504583\n",
      "  1050/50000: episode: 47, duration: 0.170s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.400030, mae: 4.423572, mean_q: 8.666507\n",
      "  1114/50000: episode: 48, duration: 0.528s, episode steps:  64, steps per second: 121, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.571182, mae: 4.605690, mean_q: 8.948801\n",
      "  1141/50000: episode: 49, duration: 0.200s, episode steps:  27, steps per second: 135, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.611568, mae: 4.761683, mean_q: 9.259294\n",
      "  1166/50000: episode: 50, duration: 0.173s, episode steps:  25, steps per second: 145, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.502079, mae: 4.884592, mean_q: 9.572412\n",
      "  1203/50000: episode: 51, duration: 0.274s, episode steps:  37, steps per second: 135, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.717625, mae: 4.976169, mean_q: 9.657581\n",
      "  1243/50000: episode: 52, duration: 0.313s, episode steps:  40, steps per second: 128, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.709266, mae: 5.153029, mean_q: 10.022002\n",
      "  1267/50000: episode: 53, duration: 0.187s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.713707, mae: 5.264630, mean_q: 10.248666\n",
      "  1284/50000: episode: 54, duration: 0.121s, episode steps:  17, steps per second: 141, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.671820, mae: 5.413292, mean_q: 10.553029\n",
      "  1304/50000: episode: 55, duration: 0.134s, episode steps:  20, steps per second: 149, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.946466, mae: 5.374625, mean_q: 10.440269\n",
      "  1368/50000: episode: 56, duration: 0.457s, episode steps:  64, steps per second: 140, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.694902, mae: 5.527015, mean_q: 10.823992\n",
      "  1394/50000: episode: 57, duration: 0.187s, episode steps:  26, steps per second: 139, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.841625, mae: 5.669627, mean_q: 11.102412\n",
      "  1415/50000: episode: 58, duration: 0.169s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.888877, mae: 5.751046, mean_q: 11.263869\n",
      "  1444/50000: episode: 59, duration: 0.213s, episode steps:  29, steps per second: 136, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.620453, mae: 5.808252, mean_q: 11.446033\n",
      "  1472/50000: episode: 60, duration: 0.199s, episode steps:  28, steps per second: 141, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.761283, mae: 5.968155, mean_q: 11.766810\n",
      "  1539/50000: episode: 61, duration: 0.527s, episode steps:  67, steps per second: 127, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.664287, mae: 6.122954, mean_q: 12.150460\n",
      "  1578/50000: episode: 62, duration: 0.305s, episode steps:  39, steps per second: 128, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.993631, mae: 6.262041, mean_q: 12.399168\n",
      "  1680/50000: episode: 63, duration: 0.802s, episode steps: 102, steps per second: 127, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.714465, mae: 6.591806, mean_q: 13.143441\n",
      "  1700/50000: episode: 64, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.901993, mae: 6.767989, mean_q: 13.428119\n",
      "  1834/50000: episode: 65, duration: 1.019s, episode steps: 134, steps per second: 132, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.774568, mae: 7.065224, mean_q: 14.127205\n",
      "  1984/50000: episode: 66, duration: 1.123s, episode steps: 150, steps per second: 134, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 0.754085, mae: 7.710231, mean_q: 15.461442\n",
      "  2013/50000: episode: 67, duration: 0.230s, episode steps:  29, steps per second: 126, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.450843, mae: 8.044406, mean_q: 16.264206\n",
      "  2112/50000: episode: 68, duration: 0.823s, episode steps:  99, steps per second: 120, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.175639, mae: 8.352901, mean_q: 16.716423\n",
      "  2156/50000: episode: 69, duration: 0.348s, episode steps:  44, steps per second: 126, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.601707, mae: 8.696967, mean_q: 17.521523\n",
      "  2302/50000: episode: 70, duration: 1.134s, episode steps: 146, steps per second: 129, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 1.423155, mae: 9.053290, mean_q: 18.196299\n",
      "  2400/50000: episode: 71, duration: 0.822s, episode steps:  98, steps per second: 119, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.232849, mae: 9.393503, mean_q: 18.957310\n",
      "  2600/50000: episode: 72, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.061264, mae: 10.017713, mean_q: 20.296057\n",
      "  2728/50000: episode: 73, duration: 1.073s, episode steps: 128, steps per second: 119, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.147617, mae: 10.831933, mean_q: 22.012203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2858/50000: episode: 74, duration: 1.295s, episode steps: 130, steps per second: 100, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.457215, mae: 11.283416, mean_q: 22.866785\n",
      "  3058/50000: episode: 75, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.417452, mae: 12.034654, mean_q: 24.454418\n",
      "  3258/50000: episode: 76, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.467060, mae: 12.861982, mean_q: 26.126135\n",
      "  3458/50000: episode: 77, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.993125, mae: 13.829668, mean_q: 28.071604\n",
      "  3609/50000: episode: 78, duration: 1.317s, episode steps: 151, steps per second: 115, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.513356, mae: 14.501846, mean_q: 29.373615\n",
      "  3726/50000: episode: 79, duration: 0.938s, episode steps: 117, steps per second: 125, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.669072, mae: 15.041501, mean_q: 30.423162\n",
      "  3926/50000: episode: 80, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.736361, mae: 15.681123, mean_q: 31.778452\n",
      "  4126/50000: episode: 81, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.218390, mae: 16.537342, mean_q: 33.544201\n",
      "  4326/50000: episode: 82, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.765707, mae: 17.189159, mean_q: 34.841179\n",
      "  4526/50000: episode: 83, duration: 1.531s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.339916, mae: 18.122032, mean_q: 36.785114\n",
      "  4717/50000: episode: 84, duration: 1.482s, episode steps: 191, steps per second: 129, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.744731, mae: 18.812401, mean_q: 38.089890\n",
      "  4907/50000: episode: 85, duration: 1.594s, episode steps: 190, steps per second: 119, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.008554, mae: 19.314447, mean_q: 39.147224\n",
      "  5074/50000: episode: 86, duration: 1.231s, episode steps: 167, steps per second: 136, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.524696, mae: 19.999014, mean_q: 40.582066\n",
      "  5239/50000: episode: 87, duration: 1.269s, episode steps: 165, steps per second: 130, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.370070, mae: 20.432791, mean_q: 41.516567\n",
      "  5429/50000: episode: 88, duration: 1.734s, episode steps: 190, steps per second: 110, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.620334, mae: 21.016268, mean_q: 42.660194\n",
      "  5583/50000: episode: 89, duration: 1.277s, episode steps: 154, steps per second: 121, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.195379, mae: 21.506268, mean_q: 43.598587\n",
      "  5783/50000: episode: 90, duration: 1.544s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.235320, mae: 22.120420, mean_q: 44.870907\n",
      "  5983/50000: episode: 91, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.346526, mae: 22.804249, mean_q: 46.152267\n",
      "  6183/50000: episode: 92, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.493690, mae: 23.429310, mean_q: 47.470943\n",
      "  6360/50000: episode: 93, duration: 1.399s, episode steps: 177, steps per second: 126, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.602102, mae: 24.174706, mean_q: 48.992760\n",
      "  6560/50000: episode: 94, duration: 1.487s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.373587, mae: 24.605633, mean_q: 49.938911\n",
      "  6760/50000: episode: 95, duration: 1.778s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.448875, mae: 25.168886, mean_q: 51.040215\n",
      "  6947/50000: episode: 96, duration: 1.471s, episode steps: 187, steps per second: 127, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.524129, mae: 25.767252, mean_q: 52.210258\n",
      "  7125/50000: episode: 97, duration: 1.361s, episode steps: 178, steps per second: 131, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 3.812891, mae: 26.220598, mean_q: 53.145771\n",
      "  7325/50000: episode: 98, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.465412, mae: 26.783175, mean_q: 54.304996\n",
      "  7525/50000: episode: 99, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.842860, mae: 27.229019, mean_q: 55.251602\n",
      "  7707/50000: episode: 100, duration: 1.368s, episode steps: 182, steps per second: 133, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.478875, mae: 27.704849, mean_q: 56.164494\n",
      "  7902/50000: episode: 101, duration: 1.469s, episode steps: 195, steps per second: 133, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.164050, mae: 28.118740, mean_q: 57.070412\n",
      "  8102/50000: episode: 102, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.603639, mae: 28.749468, mean_q: 58.377674\n",
      "  8302/50000: episode: 103, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.296055, mae: 29.281492, mean_q: 59.379128\n",
      "  8502/50000: episode: 104, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.352049, mae: 29.637304, mean_q: 60.122726\n",
      "  8702/50000: episode: 105, duration: 1.619s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.104845, mae: 30.432087, mean_q: 61.697830\n",
      "  8902/50000: episode: 106, duration: 1.652s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.099652, mae: 30.974794, mean_q: 62.716591\n",
      "  9102/50000: episode: 107, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.735323, mae: 31.270578, mean_q: 63.287987\n",
      "  9279/50000: episode: 108, duration: 1.280s, episode steps: 177, steps per second: 138, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.058685, mae: 31.856394, mean_q: 64.479210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9479/50000: episode: 109, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.452285, mae: 32.075596, mean_q: 64.907745\n",
      "  9679/50000: episode: 110, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.455661, mae: 32.505840, mean_q: 65.770599\n",
      "  9879/50000: episode: 111, duration: 1.487s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.579866, mae: 32.828617, mean_q: 66.428391\n",
      " 10069/50000: episode: 112, duration: 1.563s, episode steps: 190, steps per second: 122, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.969241, mae: 33.200359, mean_q: 67.174156\n",
      " 10266/50000: episode: 113, duration: 1.604s, episode steps: 197, steps per second: 123, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 7.461030, mae: 33.315212, mean_q: 67.347092\n",
      " 10449/50000: episode: 114, duration: 1.468s, episode steps: 183, steps per second: 125, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.918524, mae: 33.747208, mean_q: 68.322044\n",
      " 10649/50000: episode: 115, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.688785, mae: 34.171925, mean_q: 69.130814\n",
      " 10849/50000: episode: 116, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.967618, mae: 34.396679, mean_q: 69.668907\n",
      " 11049/50000: episode: 117, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.991805, mae: 34.703720, mean_q: 70.136307\n",
      " 11249/50000: episode: 118, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.633451, mae: 34.853951, mean_q: 70.550262\n",
      " 11449/50000: episode: 119, duration: 1.523s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.384078, mae: 35.129623, mean_q: 71.172691\n",
      " 11649/50000: episode: 120, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.862697, mae: 35.490528, mean_q: 71.873245\n",
      " 11844/50000: episode: 121, duration: 1.788s, episode steps: 195, steps per second: 109, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 4.377687, mae: 35.883831, mean_q: 72.700981\n",
      " 12044/50000: episode: 122, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.439363, mae: 36.114761, mean_q: 73.055092\n",
      " 12243/50000: episode: 123, duration: 1.539s, episode steps: 199, steps per second: 129, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.958446, mae: 36.161465, mean_q: 73.153427\n",
      " 12443/50000: episode: 124, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.691937, mae: 36.423878, mean_q: 73.710739\n",
      " 12643/50000: episode: 125, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.293379, mae: 36.667187, mean_q: 74.127609\n",
      " 12843/50000: episode: 126, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.885181, mae: 36.781384, mean_q: 74.423645\n",
      " 13043/50000: episode: 127, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.091952, mae: 37.057796, mean_q: 74.989212\n",
      " 13243/50000: episode: 128, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.270586, mae: 37.198395, mean_q: 75.186615\n",
      " 13443/50000: episode: 129, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.063507, mae: 37.362324, mean_q: 75.508324\n",
      " 13643/50000: episode: 130, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.307606, mae: 37.799744, mean_q: 76.613937\n",
      " 13843/50000: episode: 131, duration: 1.550s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.256176, mae: 37.721569, mean_q: 76.342133\n",
      " 14043/50000: episode: 132, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.902706, mae: 37.913372, mean_q: 76.767929\n",
      " 14243/50000: episode: 133, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.516610, mae: 38.324066, mean_q: 77.419289\n",
      " 14443/50000: episode: 134, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.583070, mae: 38.412830, mean_q: 77.670998\n",
      " 14643/50000: episode: 135, duration: 1.568s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.704910, mae: 38.725826, mean_q: 78.265388\n",
      " 14843/50000: episode: 136, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.650234, mae: 38.888206, mean_q: 78.562408\n",
      " 15043/50000: episode: 137, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.781307, mae: 39.121387, mean_q: 78.949806\n",
      " 15243/50000: episode: 138, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.114214, mae: 39.222294, mean_q: 79.228493\n",
      " 15443/50000: episode: 139, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.759195, mae: 39.096561, mean_q: 79.057396\n",
      " 15643/50000: episode: 140, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.201795, mae: 39.314762, mean_q: 79.478561\n",
      " 15843/50000: episode: 141, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.454465, mae: 39.646927, mean_q: 80.066849\n",
      " 16043/50000: episode: 142, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.072706, mae: 39.723827, mean_q: 80.200172\n",
      " 16243/50000: episode: 143, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.388750, mae: 39.824558, mean_q: 80.453407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16443/50000: episode: 144, duration: 1.733s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.766842, mae: 40.306549, mean_q: 81.361107\n",
      " 16643/50000: episode: 145, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.444304, mae: 40.162544, mean_q: 81.074036\n",
      " 16843/50000: episode: 146, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.629763, mae: 40.135223, mean_q: 81.079849\n",
      " 17032/50000: episode: 147, duration: 1.499s, episode steps: 189, steps per second: 126, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.181444, mae: 40.428200, mean_q: 81.599426\n",
      " 17222/50000: episode: 148, duration: 1.439s, episode steps: 190, steps per second: 132, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.983572, mae: 40.307175, mean_q: 81.291435\n",
      " 17422/50000: episode: 149, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.761829, mae: 40.536182, mean_q: 81.793678\n",
      " 17622/50000: episode: 150, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.875417, mae: 40.255299, mean_q: 81.225090\n",
      " 17819/50000: episode: 151, duration: 1.492s, episode steps: 197, steps per second: 132, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.011882, mae: 40.349506, mean_q: 81.422722\n",
      " 17990/50000: episode: 152, duration: 1.465s, episode steps: 171, steps per second: 117, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.865360, mae: 40.338486, mean_q: 81.393890\n",
      " 18190/50000: episode: 153, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.798891, mae: 40.409500, mean_q: 81.554169\n",
      " 18365/50000: episode: 154, duration: 1.328s, episode steps: 175, steps per second: 132, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.254495, mae: 40.354733, mean_q: 81.344803\n",
      " 18565/50000: episode: 155, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.844274, mae: 40.258228, mean_q: 81.159454\n",
      " 18765/50000: episode: 156, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.932528, mae: 39.990406, mean_q: 80.685921\n",
      " 18965/50000: episode: 157, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.778797, mae: 40.520054, mean_q: 81.667282\n",
      " 19165/50000: episode: 158, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.058832, mae: 40.956253, mean_q: 82.588844\n",
      " 19365/50000: episode: 159, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.501603, mae: 40.732632, mean_q: 82.114510\n",
      " 19565/50000: episode: 160, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.724374, mae: 40.967674, mean_q: 82.675148\n",
      " 19765/50000: episode: 161, duration: 1.544s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.255373, mae: 41.115269, mean_q: 82.782570\n",
      " 19965/50000: episode: 162, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.651029, mae: 41.373173, mean_q: 83.310158\n",
      " 20165/50000: episode: 163, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.711452, mae: 41.013443, mean_q: 82.647598\n",
      " 20365/50000: episode: 164, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.387085, mae: 40.671513, mean_q: 81.966805\n",
      " 20565/50000: episode: 165, duration: 1.671s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.537178, mae: 41.112637, mean_q: 82.928886\n",
      " 20765/50000: episode: 166, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.136007, mae: 40.911736, mean_q: 82.421745\n",
      " 20941/50000: episode: 167, duration: 1.604s, episode steps: 176, steps per second: 110, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.690866, mae: 41.357273, mean_q: 83.274544\n",
      " 21141/50000: episode: 168, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 6.084524, mae: 41.247051, mean_q: 83.038498\n",
      " 21341/50000: episode: 169, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.621106, mae: 40.855690, mean_q: 82.245644\n",
      " 21534/50000: episode: 170, duration: 1.665s, episode steps: 193, steps per second: 116, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.970726, mae: 41.028549, mean_q: 82.602798\n",
      " 21734/50000: episode: 171, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.592583, mae: 40.839027, mean_q: 82.298416\n",
      " 21934/50000: episode: 172, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.604832, mae: 41.304073, mean_q: 83.222656\n",
      " 22125/50000: episode: 173, duration: 1.546s, episode steps: 191, steps per second: 124, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.836916, mae: 40.865868, mean_q: 82.288155\n",
      " 22325/50000: episode: 174, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.575628, mae: 40.965210, mean_q: 82.506935\n",
      " 22517/50000: episode: 175, duration: 1.618s, episode steps: 192, steps per second: 119, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.945134, mae: 40.732018, mean_q: 82.088066\n",
      " 22717/50000: episode: 176, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.068721, mae: 40.659157, mean_q: 81.843361\n",
      " 22917/50000: episode: 177, duration: 1.685s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.341894, mae: 41.556396, mean_q: 83.687500\n",
      " 23117/50000: episode: 178, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.553776, mae: 41.159531, mean_q: 82.909218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23317/50000: episode: 179, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.748618, mae: 41.652462, mean_q: 83.789139\n",
      " 23517/50000: episode: 180, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.562709, mae: 41.076889, mean_q: 82.817551\n",
      " 23717/50000: episode: 181, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.603487, mae: 41.358700, mean_q: 83.247871\n",
      " 23917/50000: episode: 182, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.940566, mae: 41.076138, mean_q: 82.591721\n",
      " 24117/50000: episode: 183, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.421228, mae: 40.950878, mean_q: 82.382919\n",
      " 24317/50000: episode: 184, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.274015, mae: 41.326176, mean_q: 83.139023\n",
      " 24500/50000: episode: 185, duration: 1.803s, episode steps: 183, steps per second: 102, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.270602, mae: 41.176212, mean_q: 82.981316\n",
      " 24700/50000: episode: 186, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.510337, mae: 41.316017, mean_q: 83.040115\n",
      " 24900/50000: episode: 187, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.701725, mae: 41.638634, mean_q: 83.881439\n",
      " 25100/50000: episode: 188, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.615016, mae: 41.198586, mean_q: 83.025497\n",
      " 25300/50000: episode: 189, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.307920, mae: 41.498215, mean_q: 83.558685\n",
      " 25500/50000: episode: 190, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.135293, mae: 41.420761, mean_q: 83.347443\n",
      " 25700/50000: episode: 191, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.364908, mae: 41.317738, mean_q: 83.088669\n",
      " 25873/50000: episode: 192, duration: 1.450s, episode steps: 173, steps per second: 119, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.784764, mae: 41.361378, mean_q: 83.142067\n",
      " 26073/50000: episode: 193, duration: 1.619s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.290100, mae: 41.471756, mean_q: 83.249748\n",
      " 26273/50000: episode: 194, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.388495, mae: 41.659313, mean_q: 83.684059\n",
      " 26473/50000: episode: 195, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.577617, mae: 41.648594, mean_q: 83.690567\n",
      " 26673/50000: episode: 196, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.407714, mae: 41.771637, mean_q: 84.049026\n",
      " 26873/50000: episode: 197, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.649264, mae: 41.849262, mean_q: 84.252068\n",
      " 27073/50000: episode: 198, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.892916, mae: 41.655148, mean_q: 83.627136\n",
      " 27273/50000: episode: 199, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.198282, mae: 41.610966, mean_q: 83.693497\n",
      " 27473/50000: episode: 200, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.065233, mae: 42.216007, mean_q: 84.978584\n",
      " 27673/50000: episode: 201, duration: 1.782s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.794280, mae: 42.601757, mean_q: 85.691360\n",
      " 27873/50000: episode: 202, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.501937, mae: 41.937637, mean_q: 84.401100\n",
      " 28073/50000: episode: 203, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 10.578388, mae: 42.072906, mean_q: 84.497032\n",
      " 28273/50000: episode: 204, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.708649, mae: 41.782421, mean_q: 83.938965\n",
      " 28473/50000: episode: 205, duration: 1.673s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.439178, mae: 41.833736, mean_q: 84.181580\n",
      " 28673/50000: episode: 206, duration: 1.674s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.525056, mae: 41.716534, mean_q: 83.827438\n",
      " 28873/50000: episode: 207, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.869843, mae: 41.877541, mean_q: 84.237328\n",
      " 29073/50000: episode: 208, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.574294, mae: 41.827442, mean_q: 84.087639\n",
      " 29273/50000: episode: 209, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.709915, mae: 41.845890, mean_q: 84.046463\n",
      " 29470/50000: episode: 210, duration: 1.635s, episode steps: 197, steps per second: 120, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.561588, mae: 42.030457, mean_q: 84.433220\n",
      " 29670/50000: episode: 211, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.838027, mae: 42.222092, mean_q: 84.996025\n",
      " 29870/50000: episode: 212, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.215394, mae: 41.822304, mean_q: 84.069489\n",
      " 30070/50000: episode: 213, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.549031, mae: 41.640209, mean_q: 83.884254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30270/50000: episode: 214, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.391091, mae: 41.897884, mean_q: 84.284500\n",
      " 30470/50000: episode: 215, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.291653, mae: 41.753036, mean_q: 83.918671\n",
      " 30670/50000: episode: 216, duration: 1.709s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.038628, mae: 41.941246, mean_q: 84.313301\n",
      " 30870/50000: episode: 217, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.589867, mae: 42.198273, mean_q: 84.752342\n",
      " 31070/50000: episode: 218, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.288588, mae: 42.248993, mean_q: 84.841866\n",
      " 31270/50000: episode: 219, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.570626, mae: 41.662960, mean_q: 83.626465\n",
      " 31459/50000: episode: 220, duration: 1.925s, episode steps: 189, steps per second:  98, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.360422, mae: 41.826668, mean_q: 83.981728\n",
      " 31659/50000: episode: 221, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.723256, mae: 41.591629, mean_q: 83.506683\n",
      " 31859/50000: episode: 222, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.693786, mae: 41.992188, mean_q: 84.390762\n",
      " 32059/50000: episode: 223, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.728621, mae: 41.456547, mean_q: 83.325310\n",
      " 32259/50000: episode: 224, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.879187, mae: 41.394062, mean_q: 83.191589\n",
      " 32456/50000: episode: 225, duration: 1.676s, episode steps: 197, steps per second: 118, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 7.705198, mae: 41.261978, mean_q: 82.827484\n",
      " 32656/50000: episode: 226, duration: 1.671s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.804475, mae: 41.236988, mean_q: 82.784569\n",
      " 32856/50000: episode: 227, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.092747, mae: 41.435509, mean_q: 83.108055\n",
      " 33056/50000: episode: 228, duration: 1.673s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.256080, mae: 41.326115, mean_q: 82.871368\n",
      " 33256/50000: episode: 229, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.116860, mae: 41.600693, mean_q: 83.549652\n",
      " 33456/50000: episode: 230, duration: 1.721s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.672894, mae: 41.524086, mean_q: 83.303604\n",
      " 33656/50000: episode: 231, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.093616, mae: 41.227016, mean_q: 82.892471\n",
      " 33856/50000: episode: 232, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.537356, mae: 41.221718, mean_q: 82.708321\n",
      " 34056/50000: episode: 233, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.431208, mae: 41.199745, mean_q: 82.700745\n",
      " 34256/50000: episode: 234, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.861054, mae: 41.153877, mean_q: 82.679489\n",
      " 34456/50000: episode: 235, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.903155, mae: 41.305645, mean_q: 83.029266\n",
      " 34656/50000: episode: 236, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.790447, mae: 41.160851, mean_q: 82.687889\n",
      " 34856/50000: episode: 237, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.993995, mae: 41.522511, mean_q: 83.437325\n",
      " 35056/50000: episode: 238, duration: 1.673s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.396308, mae: 41.192753, mean_q: 82.759720\n",
      " 35256/50000: episode: 239, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.673511, mae: 41.353371, mean_q: 83.086601\n",
      " 35456/50000: episode: 240, duration: 1.721s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.697967, mae: 41.290695, mean_q: 83.025055\n",
      " 35656/50000: episode: 241, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 9.629228, mae: 41.070889, mean_q: 82.320839\n",
      " 35856/50000: episode: 242, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.392362, mae: 41.360600, mean_q: 82.999222\n",
      " 36056/50000: episode: 243, duration: 1.700s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.826110, mae: 41.640263, mean_q: 83.624908\n",
      " 36256/50000: episode: 244, duration: 1.702s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.436575, mae: 41.057362, mean_q: 82.475929\n",
      " 36456/50000: episode: 245, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.438870, mae: 41.094467, mean_q: 82.601425\n",
      " 36656/50000: episode: 246, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.041946, mae: 40.976402, mean_q: 82.392990\n",
      " 36856/50000: episode: 247, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.486995, mae: 41.244877, mean_q: 82.731857\n",
      " 37056/50000: episode: 248, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.207953, mae: 41.051216, mean_q: 82.491562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37256/50000: episode: 249, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.840280, mae: 41.175949, mean_q: 82.782753\n",
      " 37456/50000: episode: 250, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.608893, mae: 41.525146, mean_q: 83.396385\n",
      " 37656/50000: episode: 251, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.976693, mae: 41.117054, mean_q: 82.694046\n",
      " 37856/50000: episode: 252, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.668328, mae: 41.281517, mean_q: 82.945686\n",
      " 38056/50000: episode: 253, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.861845, mae: 41.157425, mean_q: 82.651085\n",
      " 38256/50000: episode: 254, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.908657, mae: 41.631107, mean_q: 83.712654\n",
      " 38456/50000: episode: 255, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.326704, mae: 41.278339, mean_q: 82.731194\n",
      " 38646/50000: episode: 256, duration: 1.624s, episode steps: 190, steps per second: 117, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 13.457131, mae: 41.342022, mean_q: 82.864609\n",
      " 38846/50000: episode: 257, duration: 1.697s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.911082, mae: 40.811562, mean_q: 82.038689\n",
      " 39046/50000: episode: 258, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.290663, mae: 40.893639, mean_q: 82.188141\n",
      " 39246/50000: episode: 259, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.267797, mae: 41.315418, mean_q: 82.965996\n",
      " 39446/50000: episode: 260, duration: 1.752s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.789898, mae: 41.193836, mean_q: 82.662430\n",
      " 39646/50000: episode: 261, duration: 1.741s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.303219, mae: 41.238277, mean_q: 82.812149\n",
      " 39846/50000: episode: 262, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.612327, mae: 41.400547, mean_q: 83.038330\n",
      " 40031/50000: episode: 263, duration: 1.564s, episode steps: 185, steps per second: 118, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5.514029, mae: 41.234940, mean_q: 82.676743\n",
      " 40231/50000: episode: 264, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.090376, mae: 41.245743, mean_q: 82.759300\n",
      " 40430/50000: episode: 265, duration: 1.676s, episode steps: 199, steps per second: 119, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.901140, mae: 40.970970, mean_q: 82.231079\n",
      " 40630/50000: episode: 266, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.447231, mae: 41.136307, mean_q: 82.504578\n",
      " 40830/50000: episode: 267, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.695279, mae: 41.100185, mean_q: 82.489197\n",
      " 41030/50000: episode: 268, duration: 2.315s, episode steps: 200, steps per second:  86, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.971134, mae: 41.054237, mean_q: 82.437012\n",
      " 41230/50000: episode: 269, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.110240, mae: 40.697891, mean_q: 81.703033\n",
      " 41430/50000: episode: 270, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 10.809622, mae: 40.813210, mean_q: 81.719818\n",
      " 41590/50000: episode: 271, duration: 1.476s, episode steps: 160, steps per second: 108, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 9.735900, mae: 40.612415, mean_q: 81.439857\n",
      " 41790/50000: episode: 272, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.879398, mae: 40.796146, mean_q: 81.949890\n",
      " 41990/50000: episode: 273, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.154130, mae: 40.827137, mean_q: 81.800514\n",
      " 42190/50000: episode: 274, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.660138, mae: 40.814507, mean_q: 81.772987\n",
      " 42390/50000: episode: 275, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.224896, mae: 40.361160, mean_q: 80.995636\n",
      " 42590/50000: episode: 276, duration: 1.749s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.554910, mae: 40.741394, mean_q: 81.768776\n",
      " 42790/50000: episode: 277, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.960193, mae: 40.979393, mean_q: 82.284515\n",
      " 42990/50000: episode: 278, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.111726, mae: 40.446606, mean_q: 81.048553\n",
      " 43190/50000: episode: 279, duration: 1.706s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.604944, mae: 40.663086, mean_q: 81.318604\n",
      " 43390/50000: episode: 280, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.745303, mae: 40.484901, mean_q: 81.224098\n",
      " 43590/50000: episode: 281, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.826849, mae: 40.487896, mean_q: 81.288261\n",
      " 43790/50000: episode: 282, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.680886, mae: 40.686707, mean_q: 81.638435\n",
      " 43990/50000: episode: 283, duration: 1.710s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.876938, mae: 40.832253, mean_q: 81.913010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44190/50000: episode: 284, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.540569, mae: 40.858086, mean_q: 82.052422\n",
      " 44390/50000: episode: 285, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.625536, mae: 40.758644, mean_q: 81.870804\n",
      " 44590/50000: episode: 286, duration: 1.733s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 11.681540, mae: 40.835213, mean_q: 81.792694\n",
      " 44790/50000: episode: 287, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.913133, mae: 40.625694, mean_q: 81.436569\n",
      " 44990/50000: episode: 288, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.716009, mae: 40.676750, mean_q: 81.507523\n",
      " 45190/50000: episode: 289, duration: 1.743s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.469310, mae: 40.561234, mean_q: 81.348495\n",
      " 45390/50000: episode: 290, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.590969, mae: 40.634197, mean_q: 81.459480\n",
      " 45590/50000: episode: 291, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 13.495808, mae: 40.524170, mean_q: 81.144119\n",
      " 45790/50000: episode: 292, duration: 1.706s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.870052, mae: 40.481106, mean_q: 81.171043\n",
      " 45990/50000: episode: 293, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.292589, mae: 40.365158, mean_q: 80.897438\n",
      " 46190/50000: episode: 294, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.105064, mae: 40.703918, mean_q: 81.795776\n",
      " 46390/50000: episode: 295, duration: 1.740s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.581424, mae: 40.312645, mean_q: 80.788467\n",
      " 46590/50000: episode: 296, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.520880, mae: 40.572609, mean_q: 81.155540\n",
      " 46790/50000: episode: 297, duration: 1.731s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.247790, mae: 40.319469, mean_q: 80.998886\n",
      " 46990/50000: episode: 298, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.943636, mae: 40.417282, mean_q: 81.055481\n",
      " 47180/50000: episode: 299, duration: 1.768s, episode steps: 190, steps per second: 107, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 9.159225, mae: 40.296345, mean_q: 80.822853\n",
      " 47380/50000: episode: 300, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.173986, mae: 40.311146, mean_q: 80.704056\n",
      " 47580/50000: episode: 301, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.095584, mae: 40.301807, mean_q: 80.749245\n",
      " 47780/50000: episode: 302, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.062303, mae: 40.353546, mean_q: 80.901825\n",
      " 47980/50000: episode: 303, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 11.553232, mae: 39.942650, mean_q: 79.909531\n",
      " 48180/50000: episode: 304, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.228766, mae: 40.051273, mean_q: 80.287331\n",
      " 48380/50000: episode: 305, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 10.016109, mae: 40.221516, mean_q: 80.577515\n",
      " 48580/50000: episode: 306, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.677835, mae: 40.121082, mean_q: 80.432297\n",
      " 48780/50000: episode: 307, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.894861, mae: 40.249222, mean_q: 80.625427\n",
      " 48980/50000: episode: 308, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.560198, mae: 39.921116, mean_q: 79.776672\n",
      " 49180/50000: episode: 309, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.439756, mae: 39.946522, mean_q: 80.137558\n",
      " 49380/50000: episode: 310, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.175932, mae: 39.899944, mean_q: 80.013733\n",
      " 49580/50000: episode: 311, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.219424, mae: 40.195412, mean_q: 80.649200\n",
      " 49780/50000: episode: 312, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.055510, mae: 40.042717, mean_q: 80.347260\n",
      " 49980/50000: episode: 313, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.713053, mae: 39.911758, mean_q: 80.139252\n",
      "done, took 425.972 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d6322dc90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, act)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9043b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 200.000, steps: 200\n",
      "Episode 18: reward: 200.000, steps: 200\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 200.000, steps: 200\n",
      "Episode 21: reward: 200.000, steps: 200\n",
      "Episode 22: reward: 200.000, steps: 200\n",
      "Episode 23: reward: 200.000, steps: 200\n",
      "Episode 24: reward: 200.000, steps: 200\n",
      "Episode 25: reward: 200.000, steps: 200\n",
      "Episode 26: reward: 200.000, steps: 200\n",
      "Episode 27: reward: 200.000, steps: 200\n",
      "Episode 28: reward: 200.000, steps: 200\n",
      "Episode 29: reward: 200.000, steps: 200\n",
      "Episode 30: reward: 200.000, steps: 200\n",
      "Episode 31: reward: 200.000, steps: 200\n",
      "Episode 32: reward: 200.000, steps: 200\n",
      "Episode 33: reward: 200.000, steps: 200\n",
      "Episode 34: reward: 200.000, steps: 200\n",
      "Episode 35: reward: 200.000, steps: 200\n",
      "Episode 36: reward: 200.000, steps: 200\n",
      "Episode 37: reward: 200.000, steps: 200\n",
      "Episode 38: reward: 200.000, steps: 200\n",
      "Episode 39: reward: 200.000, steps: 200\n",
      "Episode 40: reward: 200.000, steps: 200\n",
      "Episode 41: reward: 200.000, steps: 200\n",
      "Episode 42: reward: 200.000, steps: 200\n",
      "Episode 43: reward: 200.000, steps: 200\n",
      "Episode 44: reward: 200.000, steps: 200\n",
      "Episode 45: reward: 200.000, steps: 200\n",
      "Episode 46: reward: 200.000, steps: 200\n",
      "Episode 47: reward: 200.000, steps: 200\n",
      "Episode 48: reward: 200.000, steps: 200\n",
      "Episode 49: reward: 200.000, steps: 200\n",
      "Episode 50: reward: 200.000, steps: 200\n",
      "Episode 51: reward: 200.000, steps: 200\n",
      "Episode 52: reward: 200.000, steps: 200\n",
      "Episode 53: reward: 200.000, steps: 200\n",
      "Episode 54: reward: 200.000, steps: 200\n",
      "Episode 55: reward: 200.000, steps: 200\n",
      "Episode 56: reward: 200.000, steps: 200\n",
      "Episode 57: reward: 200.000, steps: 200\n",
      "Episode 58: reward: 200.000, steps: 200\n",
      "Episode 59: reward: 200.000, steps: 200\n",
      "Episode 60: reward: 200.000, steps: 200\n",
      "Episode 61: reward: 200.000, steps: 200\n",
      "Episode 62: reward: 200.000, steps: 200\n",
      "Episode 63: reward: 200.000, steps: 200\n",
      "Episode 64: reward: 200.000, steps: 200\n",
      "Episode 65: reward: 200.000, steps: 200\n",
      "Episode 66: reward: 200.000, steps: 200\n",
      "Episode 67: reward: 200.000, steps: 200\n",
      "Episode 68: reward: 200.000, steps: 200\n",
      "Episode 69: reward: 200.000, steps: 200\n",
      "Episode 70: reward: 200.000, steps: 200\n",
      "Episode 71: reward: 200.000, steps: 200\n",
      "Episode 72: reward: 200.000, steps: 200\n",
      "Episode 73: reward: 200.000, steps: 200\n",
      "Episode 74: reward: 200.000, steps: 200\n",
      "Episode 75: reward: 200.000, steps: 200\n",
      "Episode 76: reward: 200.000, steps: 200\n",
      "Episode 77: reward: 200.000, steps: 200\n",
      "Episode 78: reward: 200.000, steps: 200\n",
      "Episode 79: reward: 200.000, steps: 200\n",
      "Episode 80: reward: 200.000, steps: 200\n",
      "Episode 81: reward: 200.000, steps: 200\n",
      "Episode 82: reward: 200.000, steps: 200\n",
      "Episode 83: reward: 200.000, steps: 200\n",
      "Episode 84: reward: 200.000, steps: 200\n",
      "Episode 85: reward: 200.000, steps: 200\n",
      "Episode 86: reward: 200.000, steps: 200\n",
      "Episode 87: reward: 200.000, steps: 200\n",
      "Episode 88: reward: 200.000, steps: 200\n",
      "Episode 89: reward: 200.000, steps: 200\n",
      "Episode 90: reward: 200.000, steps: 200\n",
      "Episode 91: reward: 200.000, steps: 200\n",
      "Episode 92: reward: 200.000, steps: 200\n",
      "Episode 93: reward: 200.000, steps: 200\n",
      "Episode 94: reward: 200.000, steps: 200\n",
      "Episode 95: reward: 200.000, steps: 200\n",
      "Episode 96: reward: 200.000, steps: 200\n",
      "Episode 97: reward: 200.000, steps: 200\n",
      "Episode 98: reward: 200.000, steps: 200\n",
      "Episode 99: reward: 200.000, steps: 200\n",
      "Episode 100: reward: 200.000, steps: 200\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "scores =dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52ab2b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "x = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deef7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a59c3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa57fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "act = env.action_space.n\n",
    "sta = env.observation_space.shape[0]\n",
    "model = build_model(sta, act)\n",
    "dqn = build_agent(model, act)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4db22992",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a315af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env310\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 28.000, steps: 28\n",
      "Episode 2: reward: 33.000, steps: 33\n",
      "Episode 3: reward: 36.000, steps: 36\n",
      "Episode 4: reward: 46.000, steps: 46\n",
      "Episode 5: reward: 136.000, steps: 136\n"
     ]
    }
   ],
   "source": [
    "x = dqn.test(env, nb_episodes = 5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14830b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
